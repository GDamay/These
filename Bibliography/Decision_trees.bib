@article{Loh2014_decision_trees_history,
author = {Loh, Wei-Yin},
title = {Fifty Years of Classification and Regression Trees},
journal = {International Statistical Review},
volume = {82},
number = {3},
pages = {329-348},
keywords = {Classification trees, regression trees, machine learning, prediction},
doi = {https://doi.org/10.1111/insr.12016},
abstract = {AbstractFifty years have passed since the publication of the first regression tree algorithm. New techniques have added capabilities that far surpass those of the early methods. Modern classification trees can partition the data with linear splits on subsets of variables and fit nearest neighbor, kernel density, and other models in the partitions. Regression trees can fit almost every kind of traditional statistical model, including least-squares, quantile, logistic, Poisson, and proportional hazards models, as well as models for longitudinal and multiresponse data. Greater availability and affordability of software (much of which is free) have played a significant role in helping the techniques gain acceptance and popularity in the broader scientific community. This article surveys the developments and briefly reviews the key ideas behind some of the major algorithms.},
year = {2014}
}

@article{morgan1963_first_reg_tree,
author = {James N. Morgan and John A. Sonquist},
title = {Problems in the Analysis of Survey Data, and a Proposal},
journal = {Journal of the American Statistical Association},
volume = {58},
number = {302},
pages = {415--434},
year = {1963},
doi = {10.1080/01621459.1963.10500855}}

@article{Messenger1972_first_classification_tree,
author = {Robert Messenger and Lewis Mandell},
title = {A Modal Search Technique for Predictive Nominal Scale Multivariate Analysis},
journal = {Journal of the American Statistical Association},
volume = {67},
number = {340},
pages = {768--772},
year = {1972},
publisher = {ASA Website},
doi = {10.1080/01621459.1972.10481290},
}

@article{Priyanka2020_decision_trees_survey,
author = {Priyanka and Kumar, Dharmender},
title = {Decision tree classifier: a detailed survey},
journal = {International Journal of Information and Decision Sciences},
volume = {12},
number = {3},
pages = {246-269},
year = {2020},
doi = {10.1504/IJIDS.2020.108141}
}

@inproceedings{ho1995_RandomDecisionForests,
  title = {Random Decision Forests},
  booktitle = {Proceedings of 3rd {{International Conference}} on {{Document Analysis}} and {{Recognition}}},
  author = {Ho, Tin Kam},
  year = {1995},
  month = aug,
  volume = {1},
  pages = {278-282 vol.1},
  doi = {10.1109/ICDAR.1995.598994},
  urldate = {2024-09-20},
  keywords = {Classification tree analysis,Decision trees,Handwriting recognition,Hidden Markov models,Multilayer perceptrons,Optimization methods,Stochastic processes,Testing,Tin,Training data}
}

@article{fawagreh2014_RandomForestsEarly,
  title = {Random Forests: From Early Developments to Recent Advancements},
  shorttitle = {Random Forests},
  author = {Fawagreh, Khaled and Gaber, Mohamed Medhat and Elyan, Eyad},
  year = {2014},
  month = dec,
  journal = {Systems Science \& Control Engineering},
  volume = {2},
  number = {1},
  pages = {602--609},
  publisher = {Taylor \& Francis},
  issn = {null},
  doi = {10.1080/21642583.2014.956265},
  urldate = {2024-09-24}
}

@article{breiman1996_BaggingPredictors,
  title = {Bagging Predictors},
  author = {Breiman, Leo},
  year = {1996},
  month = aug,
  journal = {Machine Learning},
  volume = {24},
  number = {2},
  pages = {123--140},
  issn = {1573-0565},
  doi = {10.1007/BF00058655},
  urldate = {2024-09-24},
  langid = {english},
  keywords = {Aggregation,Artificial Intelligence,Averaging,Bootstrap,Combining}
}

@article{breiman2001_RandomForests,
  title = {Random {{Forests}}},
  author = {Breiman, Leo},
  year = {2001},
  month = oct,
  journal = {Machine Learning},
  volume = {45},
  number = {1},
  pages = {5--32},
  issn = {1573-0565},
  doi = {10.1023/A:1010933404324},
  urldate = {2024-09-24},
  langid = {english},
  keywords = {Artificial Intelligence,classification,ensemble,regression}
}

@inproceedings{bifet2009_AdaptiveLearning,
author = {Bifet, Albert and Gavald\`{a}, Ricard},
title = {Adaptive Learning from Evolving Data Streams},
year = {2009},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03915-7_22},
doi = {10.1007/978-3-642-03915-7_22},
booktitle = {Proc.\ of IDA},
pages = {249–260},
numpages = {12},
location = {Lyon, France}
}

@article{bentley1980_DecomposableSearchingProblems,
title = {Decomposable searching problems I. Static-to-dynamic transformation},
journal = {Journal of Algorithms},
volume = {1},
number = {4},
pages = {301-358},
year = {1980},
issn = {0196-6774},
doi = {https://doi.org/10.1016/0196-6774(80)90015-2},
url = {https://www.sciencedirect.com/science/article/pii/0196677480900152},
author = {Jon Louis Bentley and James B Saxe},
abstract = {Transformations that serve as tools in the design of new data structures are investigated. Specifically, general methods for converting static structures (in which all elements are known before any searches are performed) to dynamic structures (in which insertions of new elements can be mixed with searches) are studied. Three classes of such transformations are exhibited, each based on a different counting scheme for representing the integers, and a combinatorial model is used to show the optimality of many of the transformations. Issues such as online data structures and deletion of elements are also examined. To demonstrate the applicability of these tools, several new data structures that have been developed by applying the transformations are studied.}
}

@inproceedings{Domingos2000_HighSpeedStreams,
author = {Domingos, Pedro and Hulten, Geoff},
title = {Mining High-Speed Data Streams},
year = {2000},
url = {https://doi.org/10.1145/347090.347107},
doi = {10.1145/347090.347107},
booktitle = {Proc.\ of ACM KDD},
pages = {71–80},
numpages = {10},
keywords = {incremental learning, decision trees, disk-based algorithms, subsampling, Hoeffding bounds},
location = {Boston, Massachusetts, USA}
}

@article{Manapragada2022_AnEagerSplitting,
	author = {Manapragada, Chaitanya and Gomes, Heitor M. and Salehi, Mahsa and Bifet, Albert and Webb, Geoffrey I.},
	doi = {10.1007/s10618-021-00816-x},
	journal = {Data Mining and Knowledge Discovery},
	number = {2},
	pages = {566--619},
	title = {An eager splitting strategy for online decision trees in ensembles},
	volume = {36},
	year = {2022}}

@inproceedings{Domingos2001_MiningTimeSeries,
author = {Hulten, Geoff and Spencer, Laurie and Domingos, Pedro},
title = {Mining Time-Changing Data Streams},
year = {2001},
url = {https://doi.org/10.1145/502512.502529},
doi = {10.1145/502512.502529},
booktitle = {Proc.\ of ACM KDD},
pages = {97–106}
}

@inproceedings{Gama2003_HighSpeedStreams,
author = {Gama, Jo\~{a}o and Rocha, Ricardo and Medas, Pedro},
title = {Accurate Decision Trees for Mining High-Speed Data Streams},
year = {2003},
url = {https://doi.org/10.1145/956750.956813},
doi = {10.1145/956750.956813},
booktitle = {Proc.\ of ACM KDD},
pages = {523–528},
numpages = {6}
}

@inproceedings{Manapragada2018_EFDT,
author = {Manapragada, Chaitanya and Webb, Geoffrey I. and Salehi, Mahsa},
title = {Extremely Fast Decision Tree},
year = {2018},
url = {https://doi.org/10.1145/3219819.3220005},
doi = {10.1145/3219819.3220005},
booktitle = {Proc.\ of ACM KDD},
pages = {1953–1962}
}

@inproceedings{Das2019_LearnSmartWithLess,
  title     = {Learn Smart with Less: Building Better Online Decision Trees with Fewer Training Examples},
  author    = {Das, Ariyam and Wang, Jin and Gandhi, Sahil M. and Lee, Jae and Wang, Wei and Zaniolo, Carlo},
  booktitle = {Proc.\ of IJCAI},
  pages     = {2209--2215},
  year      = {2019},
  month     = {7},
  doi       = {10.24963/ijcai.2019/306}
}

@inproceedings{sun2020_SpeedingUpVeryFast,
  title     = {Speeding up Very Fast Decision Tree with Low Computational Cost},
  author    = {Sun, Jian and Jia, Hongyu and Hu, Bo and Huang, Xiao and Zhang, Hao and Wan, Hai and Zhao, Xibin},
  booktitle = {Proc.\ of IJCAI},
pages     = {1272--1278},
  year      = {2020},
  month     = {7},
  doi       = {10.24963/ijcai.2020/177}
}

@inproceedings{Haug2022_DynamicModelTree,
  author={Haug, Johannes and Broelemann, Klaus and Kasneci, Gjergji},
  title={Dynamic Model Tree for Interpretable Data Stream Learning}, 
  booktitle={Proc.\ of IEEE ICDE}, 
  year={2022},
  volume={},
  number={},
  pages={2562-2574},
  doi={10.1109/ICDE53745.2022.00237}
}

@inproceedings{Jin2003_EfficientDecisionTree,
author = {Jin, Ruoming and Agrawal, Gagan},
title = {Efficient Decision Tree Construction on Streaming Data},
year = {2003},
url = {https://doi.org/10.1145/956750.956821},
doi = {10.1145/956750.956821},
booktitle = {Proc.\ of ACM KDD},
pages = {571–576},
numpages = {6},
keywords = {decision tree, streaming data, sampling}
}

@ARTICLE{Rutkowski2013_DecisionTreesForMining,
author={Rutkowski, Leszek and Pietruczuk, Lena and Duda, Piotr and Jaworski, Maciej},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Decision Trees for Mining Data Streams Based on the McDiarmid's Bound},
year={2013},
volume={25},
number={6},
pages={1272-1279},
doi={10.1109/TKDE.2012.66}
}

@article{Barddal2020_RegularizedAndIncremental,
	author = {Barddal, Jean Paul and Enembreck, Fabr{\'\i}cio},
	doi = {10.1007/s12243-020-00782-3},
	journal = {Annals of Telecommunications},
	number = {9},
	pages = {493--503},
	title = {Regularized and incremental decision trees for data streams},
	volume = {75},
	year = {2020}
}